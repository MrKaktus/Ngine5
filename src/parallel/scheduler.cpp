/*

 Ngine v5.0
 
 Module      : Threads support.
 Requirements: none
 Description : Allows easy creation and management of threads.

*/

#include "core/log/log.h"
#include "platform/system.h"
#include "scheduler.h"

#include "utilities/random.h"

#include "core/parallel/thread.h"


#include "core/utilities/memory.h"    // allocate, TODO: move to core/memory

namespace en
{
   // Other implementations of fiber based task managers:
   //
   // NaughtyDog simple system:
   // https://www.gdcvault.com/play/1022186/Parallelizing-the-Naughty-Dog-Engine
   //
   // Google "Filament" 
   // https://github.com/google/filament/blob/master/libs/utils/src/JobSystem.cpp
   //
   // Facebook "Folly" 
   // https://github.com/facebook/folly/tree/master/folly/fibers
   //
   // General rules for Task schedulers:
   //
   // 1) IO Threads need to have ability to push Task to Scheduler, even though
   //    they are not part of Thread-Pool themselves. The same refers to Main thread.
   // 2) Only IO threads are allowed to block/sleep. Tasks executed in Thread
   //    Pool are forbidden from any kind of blocking, and should pass such work
   //    to IO threads (that will spawn new task to handle results once they are
   //    available).


   // For fiber-task sync points:
   //
   // - start execution
   // - run other task
   // - wait for other task
   // - end execution

   // TODO: Check if task local state is needed, or can it be completly skipped.


   TaskState::TaskState() :
      SharedAtomic()
   {
   }
   
   void TaskState::acquire(void)
   {
   // Performs atomic pre-increment. Equivalent to fetch_add(1) + 1
   value++;
   }

   void TaskState::release(void)
   {
   //  Performs atomic pre-decrement. Equivalent to fetch_sub(1) - 1
   value--;
   }

   bool TaskState::finished(void)
   {
   uint32 currentValue = std::atomic_load_explicit(&value, std::memory_order_relaxed);
   if (currentValue == 0)
      return true;

   return false;
   }






   // Core scheduler function. Executes between Tasks and decides which Fiber
   // should execute next, which Task to execute and how worker thread should
   // behave (for e.g. go to sleep).
   void* schedulingFunction(TaskScheduler& scheduler)
   {
   // Determine current worker thread
   uint32 threadId   = currentThreadId();
   uint32 thisWorker = threadId - scheduler.firstWorkerId;

   // Acquire handle to worker state
   Worker* workerState = &scheduler.worker[thisWorker];

   // Verify that thread executes on expected CPU core
   assert( currentCoreId() == workerState->index );

   // Execute main scheduling loop, until task scheduler terminates
   std::atomic<bool> executing = std::atomic_load_explicit(&scheduler.executing, std::memory_order_relaxed);
   while(executing)
      {
// Select task to execute

      // A) Check if there are paused tasks that can be resumed
      //    (local Fibers paused in the past, but now ready to resume)

      // TODO: Check if there are any stalled tasks (fibers) that are ready to 
      //       execute now. This fiber is done with work, so it can be put for 
      //       recycling (back to some pool) after switch.

      // TODO: !!!!

   // Switch current thread, running current fiber, to given one.
   extern void switchToFiber(Fiber& current, Fiber& fiber);




      // B) Check if there are any tasks waiting in a queue
      //    (generated on this thread, not started yet, can be stolen by other workers)
      Task* task = nullptr;
      DequeResult result = workerState->queueOfTasks.take(task);

      // C) Check if other workers have paused tasks that can be resumed
      //    (resume other worker paused Fiber on this worker)
      //
      // TODO: This requires Fibers stealing, so they need to be randomly 
      //       accessible and available for pick-up by multiple-threads in 
      //       random order. 
      //       Single-Producer Multiple-Consumers container is required.
      //       This complicates system a lot, so for now Fibers cannot be 
      //       stolen and are kept local to given worker.

      // D) Try to steal task from other randomly selected worker
      //    (task generated by other worker, not started yet)
      if (result == DequeResult::Empty)
         {
         // Iterates over all workers starting from randomly selected one, trying 
         // to steal work from any of them.
         uint32 startIndex = random(scheduler.workerThreads);
         for(uint32 i=0; i<scheduler.workerThreads; ++i)
            {
            uint32 workerId = (startIndex + i) % scheduler.workerThreads;
            if (workerId != thisWorker)
               {
               // If stealing won't succeed first time, try again (unless queue is empty)
               result = DequeResult::Abort;
               while(result != DequeResult::Empty)
                  {
                  task = nullptr;
                  result = scheduler.worker[workerId].queueOfTasks.steal(task);
                  if (result == DequeResult::Success)
                     break;
                  }
               }
            }
         }

      // Execute task 
      if (task && result == DequeResult::Success)
         {
         // Fiber executes task 
         // (it may be paused during that process, and thread can switch to other one)
         task->function(task->data);

         // Fiber exits from task function

         // Indicate that this thread finished executing task (task state may be 
         // shared by several tasks, to easily wait for all of them to finish, or 
         // in future, to allow task splitting for parallel execution).
         task->state->release();
         
         // If there are tasks waiting for this task, they now point to task state
         // that is finished (assuming it is, when shared), and they will be resumed 
         // when possible.
         
         // Release task local state
         if (task->localState)
            deallocate<TaskState>(task->state);
         
         // Release task container
         delete task;

         // Determine worker thread on which fiber finished execution
         // (fiber could have migrated between workers during task execution)
         threadId   = currentThreadId();
         thisWorker = threadId - scheduler.firstWorkerId;
         workerState = &scheduler.worker[thisWorker];
         }
      else // Worker is idle waiting for work
         {
         // If work cannot be stolen from any worker, it means that all workers
         // are executing last task or sleeping waiting for more tasks, or this
         // worker just executed last task in the system. At this point, if system
         // is not terminating, thread goes to sleep, until it will be woken up
         // to process more work (from IO threads or main thread), or shutdown.

         // Check if task scheduler is still executing
         executing = std::atomic_load_explicit(&scheduler.executing, std::memory_order_relaxed);
         if (executing)
            workerState->thread->sleep();
         }

      // Check if task scheduler is still executing
      executing = std::atomic_load_explicit(&scheduler.executing, std::memory_order_relaxed);
      }

// TODO: Worker thread is terminating, cleanup



// Fiber selects new task to execute



   // ListOfTasksStalled - need to be able to iterate over all of them, and pick only selective ones
   //                    - check if any of those can continue
   //                    - if yes, take this element out, and move in it's place last one (or other way of shortening the list and keeping it local in memory)
   //                    - put ready task on FIFO of tasks to execute
   // (one directional list would be fine here, if we want to preserve order, otherwise simple array would work fine as well)
 
   return nullptr;
   }




   constexpr uint32 InvalidWorkerID       = 0xFFFFFFFF;

   constexpr uint32 FiberStackSize        = 65536;   // 64KB
   constexpr uint32 MaxFiberStackSize     = 1048576; // 1MB
   constexpr uint32 WorkerThreadFibers    = 128;
   constexpr uint32 WorkerThreadTasks     = 256;
   constexpr uint32 MaxWorkerThreadFibers = 256;
   constexpr uint32 MaxWorkerThreadTasks  = 1024;
   constexpr uint32 MainThreadTasks       = 64;
   constexpr uint32 MaxMainThreadTasks    = 256;

   void* workerFunction(Thread* thread)
   {
   TaskScheduler& scheduler = *(TaskScheduler*)(thread->state());

   // Wait until scheduler finished initialization of all threads
   bool executing = false;
   while(executing == false) 
      {
      _mm_pause();
      executing = std::atomic_load_explicit(&scheduler.executing, std::memory_order_relaxed);
      }

   // Determine current worker thread
   uint32 threadId   = currentThreadId();
   uint32 thisWorker = threadId - scheduler.firstWorkerId;

   // Acquire handle to worker state
   Worker& workerState = *(Worker*)( &scheduler.worker[thisWorker] );

   // Finish initialization of worker thread state, by converting it to fiber
   workerState.localFibers[0] = convertToFiber();

   // Start executing tasks
   schedulingFunction(scheduler);

   // TODO: Worker thread cleanup

   // Worker thread function is not returning anything
   return nullptr;
   }

   Worker::Worker(const uint32 _index) :
      thread(nullptr),
      localFibers(nullptr),
      localTasks(MaxWorkerThreadTasks),
      localFibersWaiting(MaxWorkerThreadFibers),

      queueOfTasks(WorkerThreadTasks, MaxWorkerThreadTasks),       
    //queueOfTasksStalled(WorkerThreadFibers, MaxWorkerThreadFibers),
    //queueOfTasksWaiting(WorkerThreadFibers, MaxWorkerThreadFibers),
      index(_index),
      tasksCount(0),
      stalledTasksCount(0)
   {
   // Allocate pool of fibers 
   // (all except of fiber 0 which will be init by converting worker thread to it)
   localFibers = new std::unique_ptr<Fiber>[MaxWorkerThreadFibers];
   for(uint32 i=1; i<MaxWorkerThreadFibers; ++i)
      localFibers[i] = createFiber(FiberStackSize, MaxFiberStackSize);
   }

   Worker::~Worker()
   {
   // Release worker thread fibers
   for(uint32 i=0; i<MaxWorkerThreadFibers; ++i)
      localFibers[i] = nullptr;

   // Release fibers pool
   delete localFibers;

   // TODO:

   // Release thread object
   thread = nullptr;
   }

   TaskScheduler::TaskScheduler(const uint32 _workerThreads, const uint32 fibersPerWorker) :
      workerThreads(_workerThreads),
      firstWorkerId(0),
      worker(nullptr),
      executing(false),
      mainThreadQueue(MaxMainThreadTasks),
      mainThreadTasks(MainThreadTasks, MaxMainThreadTasks)
   {
   // Allocate per worker thread states
   worker = allocate<Worker>(workerThreads, cacheline);

   // Spawn worker threads (they will be spinning until execution flag is not set)
   for(uint32 i=0; i<workerThreads; ++i)
      {
      // Init worker state
      new (&worker[i]) Worker(i);
      worker[i].thread = startThread(workerFunction, static_cast<void*>(this));

      // Store ID of first worker thread. Implementation requires that all worker
      // threads have ID's that are part of consecutive range, so that they can
      // be easily used to index internal arrays of per thread states (after 
      // subtracting first thread ID).
      if (i == 0)
         firstWorkerId = worker[i].thread->id();
      else
         {
         assert( worker[i].thread->id() == firstWorkerId + i );
         }

      // Each worker thread is assigned to separate CPU core (physical or logical
      // depending on if Hyper-Threading cores are used) and cannot migrate between
      // those CPU cores.
      uint64 executionMask = 0;
      setBit(executionMask, static_cast<uint64>(i));
      worker[i].thread->executeOn(executionMask);
      }

   // Start worker threads execution
   std::atomic_store_explicit(&executing, true, std::memory_order_relaxed);
   }

   TaskScheduler::~TaskScheduler()
   {
   // Notify all worker threads that they should terminate. As this termination 
   // is performed in Scheduler destructor, it means that application terminates
   // immediatly. From this moment, workers won't process any new job, but 
   // terminate as soon as current task ends (or calls wait).
   //
   // TODO: Is above scenario even possible? If we've reached destructor, it
   //       means that main thread events loop was ended. By that time all tasks
   //       should be drained properly.
   //
   std::atomic_store_explicit(&executing, false, std::memory_order_relaxed);

   // Wait until all worker threads are done
   for(uint32 i=0; i<workerThreads; ++i)
      worker[i].thread->waitUntilCompleted();

   // Release worker states
   for(uint32 i=0; i<workerThreads; ++i)
      worker[i].~Worker();
   
   deallocate<Worker>(worker);
   }

   uint32 TaskScheduler::workers(void) const
   {
   return workerThreads;
   }

/* TODO:

   // Task is executed on main thread (can migrate between CPU cores)
   void TaskScheduler::runOnMainThread(TaskFunction function,
                                       void* data,
                                       TaskState* state)
   {
   // TODO: Multiple-Producers Single Consumer queue implementation is required,
   //       or simple queue protected with mutex. Those calls should be rare,
   //       and only main thread processes pushed tasks which cannot be stolen.
   }

   // Task is executed on worker thread assigned to given CPU core (cannot migrate between CPU cores)
   void TaskScheduler::runOnCore(const uint16 core,
                                 TaskFunction function,
                                 void* data,
                                 TaskState* state)
   {
   // TODO: Multiple-Producers Single Consumer queue implementation is required,
   //       or simple queue protected with mutex. Those calls should be rare,
   //       and only main thread processes pushed tasks which cannot be stolen.
   }

//*/

   void TaskScheduler::run(TaskFunction function,
                           void* data,
                           TaskState* state)
   {
   // Add task to current worker thread queue
   uint32 threadId = currentThreadId();

   // Check for special case when Task is being added by external thread not 
   // being part of Thread-Pool (for e.g. main thread or IO thread pushes task 
   // to handle incoming event).
   if (threadId < firstWorkerId || (firstWorkerId + workerThreads) < threadId)
      {
      // Add task to worker thread, that executes on current CPU core. This way
      // we're guaranteed that this worker thread execution is paused. It could 
      // be paused when it was executing task, on in rare case, when it was in
      // the middle of pushing (or grabing) task from queue.

      // TODO: Is there a safe way to push Task on a queue, when queue owner is sleeping?

      // uint32 coreId = currentCoreId();

      // task = mainThreadTasks.allocate();                   // (!) Possible hazard (multiple threads access!)
      // mainThreadQueue.push(task);                           // (!) Possible hazard (pushing while poping)

      return;
      }

   uint32 thisWorker = threadId - firstWorkerId;
       
   // Allocate task from a pool
 //Task* task = worker[thisWorker].tasks.allocate(); 
   Task* task = new Task();

   // Init task state
   task->function   = function;
   task->data       = data;
   task->state      = state;
   task->localState = false;
   if (!task->state)
      {
      // When dynamically allocated, ensure that it is aligned to cache line
      task->state = allocate<TaskState>(1, cacheline);
      new (task->state) TaskState();
      task->localState = true;
      }
   
   task->state->acquire();
   
   // Queue task for execution
   worker[thisWorker].queueOfTasks.push(task);

   // TODO: In future, add separate queue, on which core-locked tasks will be 
   //       stored for execution. Those tasks can be added by any thread, and 
   //       thus, such queue need to be of type MPSC.
   //       Worker thread owning given queue, needs to then be able to check 
   //       if there are tasks on it, in lock-free manner, so that whole task
   //       selection process is lock-free when that queue is empty. If there 
   //       are tasks on MPSC queue, then worker may lock when pooling them
   //       as it will be a rare case.
   }

   void TaskScheduler::wait(TaskState* state)
   {
   assert( state );


   // TODO: Rewrite whole Thread-Pool implementation.
   //       Take into notice interaction with I/O blocked threads.
   }

   void TaskScheduler::processMainThreadTasks(void)
   {
   Task* task = nullptr;
   while(mainThreadQueue.pop(&task))
      { 
      // Execute task
      task->function(task->data);

      // Release completed task
      task->state->release();
      if (task->localState)
         deallocate<TaskState>(task->state);

      mainThreadTasks.deallocate(*task);
      }
   }




   /* Determining if this is worker thread and which: 

   uint32 threadId = currentThreadId();

   bool workerThread = false;
   if ((threadId >= firstWorkerId) && (threadId < firstWorkerId + workerThreads))
      workerThread = true;

   uint32 workerIndex = threadId - firstWorkerId;
   */








   namespace parallel
   {

   bool Interface::create(const uint32 workers, const uint32 workerFibers, const uint32 maxWorkerTasks)
   {
   if (Scheduler)
      return true;

   Log << "Starting module: Thread-Pool Scheduler.\n";

   Scheduler = std::make_unique<TaskScheduler>(workers, workerFibers);

   return (Scheduler == nullptr) ? false : true;
   }

   }

   std::unique_ptr<parallel::Interface> Scheduler;
}





/*


// worker.cpp
// (engine internal, platform independent)

#define FibersPerThread    64
#define MaxFibersPerThread 128

namespace en
{
   WorkerState::WorkerState(const uint16 queueMaxSize) :
      readyTasks(queueMaxSize, cacheline),
      waitingTasks(queueMaxSize, cacheline),
      fibers(FibersPerThread, MaxFibersPerThread)
   {
   }
}



// Scheduler.h
// (public)

namespace en
{
   class Scheduler
      {
      bool terminating; // Indicates if system is shutting down
      
      void loop(void);          // Scheduler main loop, picks tasks, executes them, switches between fibers, manages worker thread
      void run(Task& task);
      void wait(Task& task);
      };
   
   void Scheduler::run(Task& task)
   {
   // -> push task on a queue of tasks to execute
   // -> if queue was empty, wake up worker thread
   }
   
   void Scheduler::wait(Task& task)
   {
   // -> save this fiber state, push it on queue of waiting ones
   // -> return to main loop to pick next fiber
   }

   void Scheduler::loop(void)
   {
   // One instance executing on each thread (switching between fibers in pool)
   
   // Execute tasks until whole system is not terminating
   while(!terminating)
      {
      //  -> pick task to execute
      //     -> steal task from other thread
      //        -> or sleep current thread if no tasks are available

      };
   //
   
   // push task on a queue of tasks to execute
   }

}

*/





// setcontext is one of a family of C library functions (the others being getcontext, makecontext and swapcontext
// See also: https://rethinkdb.com/blog/making-coroutines-fast/




// Main thread is I/O thread:
// - handle system events
// - execute commands restricted to main thread (delegated from other threads)
//   So it's special worker thread that executes "special tasks"
//   and does so only once per frame when it wakes up on incoming system events
//   (or no later than every Nms - this may be triggered by registering redraw or timer event).
//
// - Create bunch of worker threads (whole pool in advance)
// - Create bunch of fibers per thread (whole pool in advance)
//   ( convert worker threads to fibers if needed)
// - Assign worker threads to physical cores
//










// Work-Stealing:
// 
// deque of data 
// each deque is local to some worker thread
// worker performs push/pop operations on "bottom" of the local deque (LIFO)
// LIFO - Last In First Out -> so stack, where tasks are thrown at the top, and worker is picking them up from there as well 
// (so why it's said it picks and pushes at the "bottom"? it can cause tasks to be infinitely stalled on the "top" if new ones are constantly pushed and poped)
// other threads can steal work from the "top" - FIFO Pop 
// First In First Out - pipe like processing, the "top" pop would refer to stealing tasks from the Input side of the queue (pushed later), where worker pops work from the Output side (but it also pushes there, why??)
// 
// Implementation:
// We use only loads and stores for PushBottom and PopBottom in the common case, and transition in a lock-free manner to using a costly CAS only when a potential conflict requires processes to reach consensus.
// Only worker can update next/previous pointers in it's deque, to avoid using CAS.



// en::gpu - nested namespaces since C++17




/*
   assert( (worker < workerThreads) ||
           worker == SingleThread ||
           worker == MainThread ||
           worker == AnyThread );

   uint32 selectedWorker = InvalidWorkerID;

   // Select thread to execute task
   if (worker == SingleThread ||
       worker == AnyThread)
      {
      // Find worker thread with smallest amount of tasks to process, and assign
      // this task to it's queue. Amount of work on each worker thread queue can 
      // change during iterating over them, but thats fine - such estimation is 
      // coarse anyway, as different tasks will take different amount of time to 
      // execute (so it's hard to easily load balance amount of work in terms of
      // execution time).
      uint32 minimumTasks = MaxWorkerThreadTasks;
      for(uint32 i=0; i<workerThreads; ++i)
         {
         uint32 tasksCount = workerState[i].tasksCount; // (!) Possible hazard
         if (tasksCount < minimumTasks)
            {
            selectedWorker = i;
            minimumTasks   = tasksCount;
            if (minimumTasks == 0)
               break;
            }
         }

      assert( selectedWorker != InvalidWorkerID );
      }
   else
   if (worker < workerThreads)
      selectedWorker = worker;
*/


   // bool local = false;   // Indicates if task should be locked to current worker thread









// Other work-stealing algorithms:
// http://supertech.csail.mit.edu/papers/steal.pdf
// http://www.aladdin.cs.cmu.edu/papers/pdfs/y2000/locality_spaa00.pdf

//
// Rust, Haskell implementations:
// https://github.com/toffaletti/rust-code/blob/master/chase_lev_deque.rs
// https://github.com/ekmett/structures/blob/deque/src/Control/Concurrent/Deque.hs
// http://hackage.haskell.org/package/atomic-primops-0.4/docs/Data-Atomics-Counter-Reference.html
// 
// C++ implementation:
// https://github.com/toffaletti/chase-lev
//
// Multiple-Producer Single-Consumer Queue:
// https://github.com/samanbarghi/MPSCQ




// A dynamic-sized non-blocking work stealing deque
//
// Full paper can be found at:
// http://www.cs.tau.ac.il/~shanir/nir-pubs-web/Papers/Work_Stealing_Deque.pdf
//

/*
struct DequeNode
   {
   enum{ArraySize=8};                  // 8 size of array in a node (can tune this)
   ThreadInfo itsDataArr[ArraySize];
   DequeNode* prev;
   DequeNode* next;
   };

// Needs to fit in single CAS operand (so max 64bits)
struct BottomStruct
   {
   DequeNode* nodeP;
   int cellIndex;
   };

// Needs to fit in single CAS operand (so max 64bits)
struct TopStruct
   {
   DequeNode* nodeP;
   int cellIndex;
   int tag;           // Used to avoid ABA situation
   };

class DynamicDecque
   {
   public:
   void PushBottom(ThreadInfo theData);
   ThreadInfo PopTop(void);
   ThreadInfo PopBottom(void);

   BottomStruct Bottom;  // aligned to cache line (!)
   TopStruct    Top;     // aligned to cache line (!)
   };





void DecodeBottom(const struct BottomStruct, DequeNode*& node, int& index)
{
	// TODO: Decompress node-index pair from CAS friendly compressed representation

	// std::atomic::load( , memory_order_acquire ) ?
}

BottomStruct EncodeBottom(const DequeNode* node, const int index)
{
	// TODO: Compress node-index pair to CAS friendly compressed representation

}

void DecodeTop(const struct TopStruct, DequeNode*& node, int& index, int& tag)
{
	// TODO: Decompress node-index-tag tuple(?) from CAS friendly compressed representation

	// std::atomic::load( , memory_order_acquire ) ?
}

TopStructure EncodeTop(const DequeNode* node, int index, int tag)
{
	// TODO: Compress node-index-tag tuple(?) to CAS friendly compressed representation

}


bool EmptinessTest(const TopStructure top, const BottomStructure bottom)
{
	// TODO: !
}

void FreeOldNodeIfNeeded(void)
{
	// TODO: !
}


bool IndicateEmpty(BottomStruct bottomVal, TopStruct topVal)
{
   DequeNode* botNode  = nullptr;
   int        botIndex = 0;
   DecodeBottom(bottomVal, botNode, botIndex); 

   DequeNode* topNode  = nullptr;
   int        topIndex = 0;
   int        topTag   = 0;
   DecodeTop(topVal, topNode, topIndex, topTag);

   if (botNode == topNode &&
       (botIndex == topIndex || botIndex == (topIndex + 1)))
      return true;

   if ((botNode == topNode->next) &&
       (botIndex == 0) &&
       (topIndex == (DequeNode::ArraySize - 1)))
      return true;

   return false;
}



DynamicDecque::DynamicDecque()
{
	DequeNode* nodeA = AllocateNode();
	DequeNode* nodeB = AllocateNode();
	nodeA->next = nodeB;
	nodeB->prev = nodeA;
	Bottom = EncodeBottom(nodeA, DequeNode::ArraySize - 1);
	Top    = EncodeTop(nodeA, DequeNode::ArraySize - 1, 0);
}




void DynamicDecque::PushBottom(ThreadInfo theData)
{
DequeNode* newNode  = nullptr;
int        newIndex = 0;

DequeNode* currNode  = nullptr;
int        currIndex = 0;
DecodeBottom(Bottom, currNode, currIndex);  // Read Bottom description

currNode->itsDataArr[currIndex] = theData;   // Write data in current bottom cell  <-- std::atomic::store, memory_order_relaxed ?
if (currIndex != 0)
   {
   newNode  = currNode;
   newIndex = currIndex - 1;
   }
else // Allocate and link a new node
   {
   newNode        = AllocateNode();
   newNode->next  = currNode;
   currNode->prev = newNode;
   newIndex       = DequeNode::ArraySize - 1;
   }
Bottom = EncodeBottom(newNode, newIndex); // Update bottom
}




ThreadInfo DynamicDecque::PopTop(void)
{
   TopStructure currTop = Top;           // Read Top   <--- std::atomic::load, memory_order_acquire ?

   DequeNode* currTopNode  = nullptr;
   int        currTopIndex = 0;
   int        currTopTag   = 0;
   DecodeTop(currTop, currTopNode, currTopIndex, currTopTag);

   BottomStructure currBottom = Bottom;  // Read Bottom <--- std::atomic::load, memory_order_acquire ?

   if (EmptinessTest(currTop, currBottom))
      {
      if (currTop == Top)                // Atomic Compare ? memory_order_acquire
         return EMPTY;
      else
         return ABORT;
      }

   DequeNode* newTopNode = nullptr;
   int        newTopIndex = 0;
   int        newTopTag   = 0;
   if (currTopIndex != 0) // if deque isn't empty calculate next top pointer
      { // stay at current node:
      newTopNode  = currTopNode;
      newTopIndex = currTopIndex - 1;
      newTopTag   = currTopTag;
      }
   else // move to next node and update tag:
      {
      newTopNode  = currTopNode->prev;
      newTopIndex = DequeNode::ArraySize - 1;
      newTopTag   = currTopTag + 1;
      }

   ThreadInfo retVal = currTopNode->itsDataArr[currTopIndex];    // Read value

   TopStructure newTopVal;
   EncodeTop(newTopVal, newTopNode, newTopIndex, newTopTag);

   if (CAS(&Top, currTop, newTopVal))   // Try to update Top using CAS
      {
      FreeOldNodeIfNeeded(void);
      return retVal;
      }
   else
      {
      return ABORT;
      }
}



ThreadInfo DynamicDecque::PopBottom(void)
{
   BottomStruct oldBottom = Bottom; // Read Bottom description  <-- std::atomicload, memory_order_acquire

   DequeNode* oldBotNode  = nullptr;
   int        oldBotIndex = 0;

   DecodeBottom(oldBottom, oldBotNode, oldBotIndex);  

   if (oldBotIndex != (DequeNode::ArraySize - 1))
      {
      newBotNode  = oldBotNode;
      newBotIndex = oldBotIndex + 1;
      }
   else
      {
      newBotNode  = oldBotNode->next;
      newBotIndex = 0;
      }
	
   ThreadInfo retVal = newBotNode->itsDataArr[newBotIndex];  // Read data to be popped

   Bottom = EncodeBottom(newBotNode, newBotIndex); // Update bottom

   TopStructure currTop = Top;           // Read Top   <--- std::atomic::load, memory_order_acquire ?

   DequeNode* currTopNode  = nullptr;
   int        currTopIndex = 0;
   int        currTopTag   = 0;
   DecodeTop(currTop, currTopNode, currTopIndex, currTopTag);

   // Case 1: if Top has crossed Bottom
   if (oldBotNode == currTopNode &&
       oldBotIndex == currTopIndex)
      {
      // Return bottom to its old position
      Bottom = EncodeBottom(oldBotNode, oldBotIndex);
      return EMPTY;
      }
   else // Case 2: when popping the last entry in the deque (ie. deque is empty after the update of bottom)
   if (newBotNode == currTopNode &&
       newBotIndex == currTopIndex)
      {
      // Try to update Top's tag so no concurrent PopTop operation will also pop the same entry:
      ThreadInfo newTopVal = EncodeTop(currTopNode, currTopIndex, currTopTag + 1);
      if (CAS(&Top, currTop, newTopVal))
         {
         FreeOldNodeIfNeeded();
         return retVal;
         }
      else // if CAS failed (i.e. a concurrent PopTop operation already popped the last entry)
         {
         // Return bottom to it's old position
         Bottom = EncodeBottom(oldBotNode, oldBotIndex);
         return EMPTY;
         }
      }
   else // Case 3: Regular case (i.e. there was at least one entry in the deque <<after>> bottom's update)
      {
      FreeOldNodeIfNeeded();
      return retVal;
      }
}

*/
